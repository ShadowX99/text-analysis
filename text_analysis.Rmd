---
title: "text analysis"
author: "Matt Sterkel"
date: "November 29, 2017"
output: html_document
---

```{r, message=FALSE,warning=FALSE}
library(twitteR)
library(dplyr)
library(purrr)
library(stringr)
library(tm)
```

```{r}

api_key<- "xxxxx"
api_secret <- "xxxxx"
access_token <- "xxxxx"
access_token_secret <- "xxxxx"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
```




```{r}

prat_tweets <- userTimeline("prattprattpratt", n = 250)

oprah_tweets <- userTimeline("Oprah", n = 250)

neil_tweets <- userTimeline("neiltyson", n = 250)
 
mar_tweets <- userTimeline("billmaher", n = 250)

kutch_tweets <- userTimeline("aplusk", n = 250)
  


tweets<- tbl_df(map_df(c(prat_tweets,oprah_tweets,neil_tweets,
                  mar_tweets,kutch_tweets),as.data.frame))  
  
write.csv(tweets, file="tweets.csv", row.names=FALSE)  

```



```{r}

setwd("C:/Users/mateo/Documents/Repo/text_analysis")

tweets<-read.csv("tweets.csv")
```


```{r}
twitterCorpus <-Corpus(VectorSource(tweets$text))

inspect(twitterCorpus[1:10])

  twitterCorpus<- tm_map(twitterCorpus, content_transformer(tolower))
    twitterCorpus<- tm_map(twitterCorpus,removeWords,stopwords("en"))
      twitterCorpus<- tm_map( twitterCorpus,removeNumbers)
        twitterCorpus<- tm_map( twitterCorpus,removePunctuation)
           
            removeURL<- function(x) gsub("http[[:alnum:]]*", "", x)   
              twitterCorpus<- tm_map(twitterCorpus,content_transformer(removeURL))
                
            removeURL<- function(x) gsub("edua[[:alnum:]]*", "", x)   
              twitterCorpus<- tm_map(twitterCorpus,content_transformer(removeURL))
              
  # remove non "American standard code for information interchange (curly quotes and ellipsis)"
  #  using function from package "textclean"            
                           
removeNonAscii<-function(x) textclean::replace_non_ascii(x) 
  twitterCorpus<-tm_map(twitterCorpus,content_transformer(removeNonAscii))
              
              twitterCorpus<- tm_map(twitterCorpus,removeWords,c("amp","ufef",
                                                                   "ufeft","uufefuufefuufef","uufef","s"))  
              
          twitterCorpus<- tm_map(twitterCorpus,stripWhitespace)
          
inspect(twitterCorpus[1:10])
 


# stem corpus after sentiment, but before cluster analysis


```


```{r}
dtm<-DocumentTermMatrix(twitterCorpus)
mat<-as.matrix(dtm)
d<-dist(mat)
groups<-hclust(d,method="ward.D")
plot(groups,hang=-1)
cut<-cutree(groups,k=5)
plot(cut)
newMat<-dplyr::bind_cols(tweets,data.frame(cut))

table(newMat$screenName,newMat$cut)
```


